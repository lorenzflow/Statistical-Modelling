# Model Diagnostics Cheatsheet

During my MSc we used to have an assessed project every monday. With a total of 9h to complete a project you can imagine that speed matters. 
So usually I would write a quick summary of my prep the day before with practical tips and references to useful sources. 
This is one of those summaries on model diagnostics for normal linear models (NLM).

In the following let Y be the response, p the number of covariates, n the number of observations, and X the design matrix. 
Note: I wrote this markdown very quickly, so notation is not very clear for now - might update it later

## Coefficient of determination $R^2$
- A measure of the goodness of fit for a NLM
- Larger models with more paramters will have a smaller residual sum of squares (RSS)
For models with an intercept we have: $$R^2 = 1 - \frac{RSS}{\sum_{i=1}^n (y_i-\bar{y}_i)^2}$$
which means $0 \leq R^2 \leq 1$
- The $R^2$ can be interpreted as the proportion of variance in the data that is explained by the model

## Adjusted $R^2$
$$\bar{R}^2 = 1 - \frac{RSS}{\sum_{i=1}^n (y_i-\bar{y}_i)^2} \frac{n-1}{n-p}$$
- takes into account the number of model parameters
- can be negative
- tries to balance out the effect of decreasing RSS when increasing the number of parameters

## Outliers

Potential causes of outliers can be: errors in data recording, data is a mixture of different populations, a bad model
For 2 or only 1 variable outliers can easily be detected for example via: boxplots, scatterplots, residuals.

### Residuals

- $e = Y-\hat{Y} = (I_n - P)Y$, where $P=X(X^TX)^{-1}X^T$
- $cov(e) = \sigma^2(I_n-P)$ and $E(e)=0$ and it follows $e\sim N(0,\sigma^2(I_n-P))$
- 
